10/26/2019 16:03:00 - INFO - root -   {'run_text': 'PICO-Classification-By-Bert', 'train_size': -1, 'val_size': -1, 'log_path': WindowsPath('logs'), 'full_data_dir': WindowsPath('data'), 'data_dir': WindowsPath('data'), 'task_name': 'PICO-Classification', 'no_cuda': False, 'bert_model': WindowsPath('pretrained'), 'output_dir': WindowsPath('models/output'), 'max_seq_length': 60, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 20, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': True, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert', 'multi_gpu': False}
10/26/2019 16:03:00 - INFO - transformers.tokenization_utils -   Model name 'pretrained' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'pretrained' is a path or url to a directory containing tokenizer files.
10/26/2019 16:03:00 - INFO - transformers.tokenization_utils -   Didn't find file pretrained\added_tokens.json. We won't load it.
10/26/2019 16:03:00 - INFO - transformers.tokenization_utils -   Didn't find file pretrained\special_tokens_map.json. We won't load it.
10/26/2019 16:03:00 - INFO - transformers.tokenization_utils -   Didn't find file pretrained\tokenizer_config.json. We won't load it.
10/26/2019 16:03:00 - INFO - transformers.tokenization_utils -   loading file pretrained\vocab.txt
10/26/2019 16:03:00 - INFO - transformers.tokenization_utils -   loading file None
10/26/2019 16:03:00 - INFO - transformers.tokenization_utils -   loading file None
10/26/2019 16:03:00 - INFO - transformers.tokenization_utils -   loading file None
10/26/2019 16:03:00 - INFO - pytorch_transformers.tokenization_utils -   Model name 'models\output\model_out' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'models\output\model_out' is a path or url to a directory containing tokenizer files.
10/26/2019 16:03:00 - INFO - pytorch_transformers.tokenization_utils -   loading file models\output\model_out\vocab.txt
10/26/2019 16:03:00 - INFO - pytorch_transformers.tokenization_utils -   loading file models\output\model_out\added_tokens.json
10/26/2019 16:03:00 - INFO - pytorch_transformers.tokenization_utils -   loading file models\output\model_out\special_tokens_map.json
10/26/2019 16:03:00 - INFO - pytorch_transformers.tokenization_utils -   loading file models\output\model_out\tokenizer_config.json
10/26/2019 16:03:00 - INFO - pytorch_transformers.modeling_utils -   loading configuration file models\output\model_out\config.json
10/26/2019 16:03:00 - INFO - pytorch_transformers.modeling_utils -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 3,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/26/2019 16:03:01 - INFO - pytorch_transformers.modeling_utils -   loading weights file models\output\model_out\pytorch_model.bin
10/26/2019 16:03:19 - INFO - root -   Writing example 0 of 1435
