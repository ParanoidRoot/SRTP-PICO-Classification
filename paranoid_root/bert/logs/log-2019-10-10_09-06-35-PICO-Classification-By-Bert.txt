10/10/2019 09:06:35 - INFO - root -   {'run_text': 'PICO-Classification-By-Bert', 'train_size': -1, 'val_size': -1, 'log_path': WindowsPath('logs'), 'full_data_dir': WindowsPath('data'), 'data_dir': WindowsPath('data'), 'task_name': 'PICO-Classification', 'no_cuda': False, 'bert_model': WindowsPath('pretrained'), 'output_dir': WindowsPath('models/output'), 'max_seq_length': 60, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 20, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': True, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert', 'multi_gpu': False}
10/10/2019 09:06:35 - INFO - transformers.tokenization_utils -   Model name 'pretrained' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'pretrained' is a path or url to a directory containing tokenizer files.
10/10/2019 09:06:35 - INFO - transformers.tokenization_utils -   Didn't find file pretrained\added_tokens.json. We won't load it.
10/10/2019 09:06:35 - INFO - transformers.tokenization_utils -   Didn't find file pretrained\special_tokens_map.json. We won't load it.
10/10/2019 09:06:35 - INFO - transformers.tokenization_utils -   Didn't find file pretrained\tokenizer_config.json. We won't load it.
10/10/2019 09:06:35 - INFO - transformers.tokenization_utils -   loading file pretrained\vocab.txt
10/10/2019 09:06:35 - INFO - transformers.tokenization_utils -   loading file None
10/10/2019 09:06:35 - INFO - transformers.tokenization_utils -   loading file None
10/10/2019 09:06:35 - INFO - transformers.tokenization_utils -   loading file None
10/10/2019 09:06:35 - INFO - root -   Loading features from cached file data\cache\cached_bert_train_multi_label_60
10/10/2019 09:06:35 - INFO - root -   Loading features from cached file data\cache\cached_bert_dev_multi_label_60
10/10/2019 09:06:35 - INFO - pytorch_transformers.modeling_utils -   loading configuration file pretrained\config.json
10/10/2019 09:06:35 - INFO - pytorch_transformers.modeling_utils -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 3,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/10/2019 09:06:35 - INFO - pytorch_transformers.modeling_utils -   loading weights file pretrained\pytorch_model.bin
10/10/2019 09:06:39 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
10/10/2019 09:06:39 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
10/10/2019 09:06:43 - INFO - root -   ***** Running training *****
10/10/2019 09:06:43 - INFO - root -     Num examples = 1291
10/10/2019 09:06:43 - INFO - root -     Num Epochs = 20
10/10/2019 09:06:43 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 8
10/10/2019 09:06:43 - INFO - root -     Gradient Accumulation steps = 1
10/10/2019 09:06:43 - INFO - root -     Total optimization steps = 3240
10/10/2019 09:08:25 - INFO - root -   Running evaluation
10/10/2019 09:08:25 - INFO - root -     Num examples = 144
10/10/2019 09:08:25 - INFO - root -     Batch size = 16
10/10/2019 09:08:26 - INFO - root -   eval_loss after epoch 1: 0.6676767733361986: 
10/10/2019 09:08:26 - INFO - root -   eval_accuracy_thresh after epoch 1: 0.6597222089767456: 
10/10/2019 09:08:26 - INFO - root -   eval_roc_auc after epoch 1: 0.5613631512794365: 
10/10/2019 09:08:26 - INFO - root -   eval_fbeta after epoch 1: 0.7210497260093689: 
10/10/2019 09:08:26 - INFO - root -   eval_accuracy_multilabel after epoch 1: 0.3611111111111111: 
10/10/2019 09:08:27 - INFO - root -   lr after epoch 1: 1.62e-05
10/10/2019 09:08:27 - INFO - root -   train_loss after epoch 1: 0.6904312688627361
10/10/2019 09:08:27 - INFO - root -   

10/10/2019 09:10:04 - INFO - root -   Running evaluation
10/10/2019 09:10:04 - INFO - root -     Num examples = 144
10/10/2019 09:10:04 - INFO - root -     Batch size = 16
10/10/2019 09:10:05 - INFO - root -   eval_loss after epoch 2: 0.6217124395900302: 
10/10/2019 09:10:05 - INFO - root -   eval_accuracy_thresh after epoch 2: 0.6597222089767456: 
10/10/2019 09:10:05 - INFO - root -   eval_roc_auc after epoch 2: 0.6920696279080798: 
10/10/2019 09:10:05 - INFO - root -   eval_fbeta after epoch 2: 0.725183367729187: 
10/10/2019 09:10:05 - INFO - root -   eval_accuracy_multilabel after epoch 2: 0.4166666666666667: 
10/10/2019 09:10:05 - INFO - root -   lr after epoch 2: 3.24e-05
10/10/2019 09:10:05 - INFO - root -   train_loss after epoch 2: 0.6525202445041987
10/10/2019 09:10:05 - INFO - root -   

10/10/2019 09:11:42 - INFO - root -   Running evaluation
10/10/2019 09:11:42 - INFO - root -     Num examples = 144
10/10/2019 09:11:42 - INFO - root -     Batch size = 16
10/10/2019 09:11:43 - INFO - root -   eval_loss after epoch 3: 0.5668813387552897: 
10/10/2019 09:11:43 - INFO - root -   eval_accuracy_thresh after epoch 3: 0.7291666865348816: 
10/10/2019 09:11:43 - INFO - root -   eval_roc_auc after epoch 3: 0.8081319515260749: 
10/10/2019 09:11:43 - INFO - root -   eval_fbeta after epoch 3: 0.8098846077919006: 
10/10/2019 09:11:43 - INFO - root -   eval_accuracy_multilabel after epoch 3: 0.6180555555555556: 
10/10/2019 09:11:43 - INFO - root -   lr after epoch 3: 4.86e-05
10/10/2019 09:11:43 - INFO - root -   train_loss after epoch 3: 0.6036521155893066
10/10/2019 09:11:43 - INFO - root -   

10/10/2019 09:13:22 - INFO - root -   Running evaluation
10/10/2019 09:13:22 - INFO - root -     Num examples = 144
10/10/2019 09:13:22 - INFO - root -     Batch size = 16
10/10/2019 09:13:23 - INFO - root -   eval_loss after epoch 4: 0.4832206202877892: 
10/10/2019 09:13:23 - INFO - root -   eval_accuracy_thresh after epoch 4: 0.8425925970077515: 
10/10/2019 09:13:23 - INFO - root -   eval_roc_auc after epoch 4: 0.9043090568454004: 
10/10/2019 09:13:23 - INFO - root -   eval_fbeta after epoch 4: 0.8243497014045715: 
10/10/2019 09:13:23 - INFO - root -   eval_accuracy_multilabel after epoch 4: 0.7986111111111112: 
10/10/2019 09:13:23 - INFO - root -   lr after epoch 4: 4.964092127401103e-05
10/10/2019 09:13:23 - INFO - root -   train_loss after epoch 4: 0.5393720127550172
10/10/2019 09:13:23 - INFO - root -   

10/10/2019 09:14:59 - INFO - root -   Running evaluation
10/10/2019 09:14:59 - INFO - root -     Num examples = 144
10/10/2019 09:14:59 - INFO - root -     Batch size = 16
10/10/2019 09:15:00 - INFO - root -   eval_loss after epoch 5: 0.4070342779159546: 
10/10/2019 09:15:00 - INFO - root -   eval_accuracy_thresh after epoch 5: 0.8865740895271301: 
10/10/2019 09:15:00 - INFO - root -   eval_roc_auc after epoch 5: 0.936549908696374: 
10/10/2019 09:15:00 - INFO - root -   eval_fbeta after epoch 5: 0.8602293133735657: 
10/10/2019 09:15:00 - INFO - root -   eval_accuracy_multilabel after epoch 5: 0.8541666666666666: 
10/10/2019 09:15:00 - INFO - root -   lr after epoch 5: 4.8437374170450344e-05
10/10/2019 09:15:00 - INFO - root -   train_loss after epoch 5: 0.45756232572926414
10/10/2019 09:15:00 - INFO - root -   

10/10/2019 09:16:37 - INFO - root -   Running evaluation
10/10/2019 09:16:37 - INFO - root -     Num examples = 144
10/10/2019 09:16:37 - INFO - root -     Batch size = 16
10/10/2019 09:16:38 - INFO - root -   eval_loss after epoch 6: 0.3436334431171417: 
10/10/2019 09:16:38 - INFO - root -   eval_accuracy_thresh after epoch 6: 0.9050925970077515: 
10/10/2019 09:16:38 - INFO - root -   eval_roc_auc after epoch 6: 0.9494272772547253: 
10/10/2019 09:16:38 - INFO - root -   eval_fbeta after epoch 6: 0.8899912238121033: 
10/10/2019 09:16:38 - INFO - root -   eval_accuracy_multilabel after epoch 6: 0.8541666666666666: 
10/10/2019 09:16:38 - INFO - root -   lr after epoch 6: 4.642754199508835e-05
10/10/2019 09:16:38 - INFO - root -   train_loss after epoch 6: 0.3882055014003942
10/10/2019 09:16:38 - INFO - root -   

10/10/2019 09:18:14 - INFO - root -   Running evaluation
10/10/2019 09:18:14 - INFO - root -     Num examples = 144
10/10/2019 09:18:14 - INFO - root -     Batch size = 16
10/10/2019 09:18:16 - INFO - root -   eval_loss after epoch 7: 0.2971346941259172: 
10/10/2019 09:18:16 - INFO - root -   eval_accuracy_thresh after epoch 7: 0.9212962985038757: 
10/10/2019 09:18:16 - INFO - root -   eval_roc_auc after epoch 7: 0.957063580525055: 
10/10/2019 09:18:16 - INFO - root -   eval_fbeta after epoch 7: 0.9061949253082275: 
10/10/2019 09:18:16 - INFO - root -   eval_accuracy_multilabel after epoch 7: 0.8680555555555556: 
10/10/2019 09:18:16 - INFO - root -   lr after epoch 7: 4.36805663534665e-05
10/10/2019 09:18:16 - INFO - root -   train_loss after epoch 7: 0.3306962924974936
10/10/2019 09:18:16 - INFO - root -   

10/10/2019 09:19:52 - INFO - root -   Running evaluation
10/10/2019 09:19:52 - INFO - root -     Num examples = 144
10/10/2019 09:19:52 - INFO - root -     Batch size = 16
10/10/2019 09:19:53 - INFO - root -   eval_loss after epoch 8: 0.2672323551442888: 
10/10/2019 09:19:53 - INFO - root -   eval_accuracy_thresh after epoch 8: 0.9305555820465088: 
10/10/2019 09:19:53 - INFO - root -   eval_roc_auc after epoch 8: 0.961735480351934: 
10/10/2019 09:19:53 - INFO - root -   eval_fbeta after epoch 8: 0.9151234030723572: 
10/10/2019 09:19:53 - INFO - root -   eval_accuracy_multilabel after epoch 8: 0.8819444444444444: 
10/10/2019 09:19:53 - INFO - root -   lr after epoch 8: 4.029094782557015e-05
10/10/2019 09:19:53 - INFO - root -   train_loss after epoch 8: 0.289916941413173
10/10/2019 09:19:53 - INFO - root -   

10/10/2019 09:21:32 - INFO - root -   Running evaluation
10/10/2019 09:21:32 - INFO - root -     Num examples = 144
10/10/2019 09:21:32 - INFO - root -     Batch size = 16
10/10/2019 09:21:33 - INFO - root -   eval_loss after epoch 9: 0.24364495442973244: 
10/10/2019 09:21:33 - INFO - root -   eval_accuracy_thresh after epoch 9: 0.9351851940155029: 
10/10/2019 09:21:33 - INFO - root -   eval_roc_auc after epoch 9: 0.9653994830080396: 
10/10/2019 09:21:33 - INFO - root -   eval_fbeta after epoch 9: 0.9116511940956116: 
10/10/2019 09:21:33 - INFO - root -   eval_accuracy_multilabel after epoch 9: 0.8888888888888888: 
10/10/2019 09:21:33 - INFO - root -   lr after epoch 9: 3.63752949870131e-05
10/10/2019 09:21:33 - INFO - root -   train_loss after epoch 9: 0.25961995741099486
10/10/2019 09:21:33 - INFO - root -   

10/10/2019 09:23:10 - INFO - root -   Running evaluation
10/10/2019 09:23:10 - INFO - root -     Num examples = 144
10/10/2019 09:23:10 - INFO - root -     Batch size = 16
10/10/2019 09:23:11 - INFO - root -   eval_loss after epoch 10: 0.22538627021842533: 
10/10/2019 09:23:11 - INFO - root -   eval_accuracy_thresh after epoch 10: 0.9351851940155029: 
10/10/2019 09:23:11 - INFO - root -   eval_roc_auc after epoch 10: 0.9685061778167762: 
10/10/2019 09:23:11 - INFO - root -   eval_fbeta after epoch 10: 0.9255400896072388: 
10/10/2019 09:23:11 - INFO - root -   eval_accuracy_multilabel after epoch 10: 0.8958333333333334: 
10/10/2019 09:23:11 - INFO - root -   lr after epoch 10: 3.2068312878006955e-05
10/10/2019 09:23:11 - INFO - root -   train_loss after epoch 10: 0.23395159284089817
10/10/2019 09:23:11 - INFO - root -   

10/10/2019 09:24:47 - INFO - root -   Running evaluation
10/10/2019 09:24:47 - INFO - root -     Num examples = 144
10/10/2019 09:24:47 - INFO - root -     Batch size = 16
10/10/2019 09:24:49 - INFO - root -   eval_loss after epoch 11: 0.21058163212405312: 
10/10/2019 09:24:49 - INFO - root -   eval_accuracy_thresh after epoch 11: 0.9351851940155029: 
10/10/2019 09:24:49 - INFO - root -   eval_roc_auc after epoch 11: 0.9744112694761305: 
10/10/2019 09:24:49 - INFO - root -   eval_fbeta after epoch 11: 0.9290122985839844: 
10/10/2019 09:24:49 - INFO - root -   eval_accuracy_multilabel after epoch 11: 0.8958333333333334: 
10/10/2019 09:24:49 - INFO - root -   lr after epoch 11: 2.7518168923526534e-05
10/10/2019 09:24:49 - INFO - root -   train_loss after epoch 11: 0.2110305587174716
10/10/2019 09:24:49 - INFO - root -   

10/10/2019 09:26:25 - INFO - root -   Running evaluation
10/10/2019 09:26:25 - INFO - root -     Num examples = 144
10/10/2019 09:26:25 - INFO - root -     Batch size = 16
10/10/2019 09:26:26 - INFO - root -   eval_loss after epoch 12: 0.19495158394177756: 
10/10/2019 09:26:26 - INFO - root -   eval_accuracy_thresh after epoch 12: 0.9375: 
10/10/2019 09:26:26 - INFO - root -   eval_roc_auc after epoch 12: 0.9743519814072614: 
10/10/2019 09:26:26 - INFO - root -   eval_fbeta after epoch 12: 0.9386574625968933: 
10/10/2019 09:26:26 - INFO - root -   eval_accuracy_multilabel after epoch 12: 0.9027777777777778: 
10/10/2019 09:26:26 - INFO - root -   lr after epoch 12: 2.2881395724807943e-05
10/10/2019 09:26:26 - INFO - root -   train_loss after epoch 12: 0.19415158714041297
10/10/2019 09:26:26 - INFO - root -   

10/10/2019 09:28:03 - INFO - root -   Running evaluation
10/10/2019 09:28:03 - INFO - root -     Num examples = 144
10/10/2019 09:28:03 - INFO - root -     Batch size = 16
10/10/2019 09:28:04 - INFO - root -   eval_loss after epoch 13: 0.1838681466049618: 
10/10/2019 09:28:04 - INFO - root -   eval_accuracy_thresh after epoch 13: 0.9513888955116272: 
10/10/2019 09:28:04 - INFO - root -   eval_roc_auc after epoch 13: 0.9746602793653805: 
10/10/2019 09:28:04 - INFO - root -   eval_fbeta after epoch 13: 0.9417438507080078: 
10/10/2019 09:28:04 - INFO - root -   eval_accuracy_multilabel after epoch 13: 0.9166666666666666: 
10/10/2019 09:28:04 - INFO - root -   lr after epoch 13: 1.831750607472064e-05
10/10/2019 09:28:04 - INFO - root -   train_loss after epoch 13: 0.18456068018704286
10/10/2019 09:28:04 - INFO - root -   

10/10/2019 09:29:40 - INFO - root -   Running evaluation
10/10/2019 09:29:40 - INFO - root -     Num examples = 144
10/10/2019 09:29:40 - INFO - root -     Batch size = 16
10/10/2019 09:29:42 - INFO - root -   eval_loss after epoch 14: 0.1780558716919687: 
10/10/2019 09:29:42 - INFO - root -   eval_accuracy_thresh after epoch 14: 0.9537037014961243: 
10/10/2019 09:29:42 - INFO - root -   eval_roc_auc after epoch 14: 0.9756088884672849: 
10/10/2019 09:29:42 - INFO - root -   eval_fbeta after epoch 14: 0.9386574625968933: 
10/10/2019 09:29:42 - INFO - root -   eval_accuracy_multilabel after epoch 14: 0.9166666666666666: 
10/10/2019 09:29:42 - INFO - root -   lr after epoch 14: 1.398350544953736e-05
10/10/2019 09:29:42 - INFO - root -   train_loss after epoch 14: 0.17023344770257856
10/10/2019 09:29:42 - INFO - root -   

10/10/2019 09:31:18 - INFO - root -   Running evaluation
10/10/2019 09:31:18 - INFO - root -     Num examples = 144
10/10/2019 09:31:18 - INFO - root -     Batch size = 16
10/10/2019 09:31:19 - INFO - root -   eval_loss after epoch 15: 0.17398434629042944: 
10/10/2019 09:31:19 - INFO - root -   eval_accuracy_thresh after epoch 15: 0.9537037014961243: 
10/10/2019 09:31:19 - INFO - root -   eval_roc_auc after epoch 15: 0.9760357625631417: 
10/10/2019 09:31:19 - INFO - root -   eval_fbeta after epoch 15: 0.9386574625968933: 
10/10/2019 09:31:19 - INFO - root -   eval_accuracy_multilabel after epoch 15: 0.9236111111111112: 
10/10/2019 09:31:19 - INFO - root -   lr after epoch 15: 1.0028490756609971e-05
10/10/2019 09:31:19 - INFO - root -   train_loss after epoch 15: 0.16515195194953752
10/10/2019 09:31:19 - INFO - root -   

10/10/2019 09:32:56 - INFO - root -   Running evaluation
10/10/2019 09:32:56 - INFO - root -     Num examples = 144
10/10/2019 09:32:56 - INFO - root -     Batch size = 16
10/10/2019 09:32:57 - INFO - root -   eval_loss after epoch 16: 0.1675194808178478: 
10/10/2019 09:32:57 - INFO - root -   eval_accuracy_thresh after epoch 16: 0.9537037014961243: 
10/10/2019 09:32:57 - INFO - root -   eval_roc_auc after epoch 16: 0.9765337823416417: 
10/10/2019 09:32:57 - INFO - root -   eval_fbeta after epoch 16: 0.9429011940956116: 
10/10/2019 09:32:57 - INFO - root -   eval_accuracy_multilabel after epoch 16: 0.9305555555555556: 
10/10/2019 09:32:57 - INFO - root -   lr after epoch 16: 6.588521150111254e-06
10/10/2019 09:32:57 - INFO - root -   train_loss after epoch 16: 0.1562968395466422
10/10/2019 09:32:57 - INFO - root -   

10/10/2019 09:34:34 - INFO - root -   Running evaluation
10/10/2019 09:34:34 - INFO - root -     Num examples = 144
10/10/2019 09:34:34 - INFO - root -     Batch size = 16
10/10/2019 09:34:35 - INFO - root -   eval_loss after epoch 17: 0.16505259606573316: 
10/10/2019 09:34:35 - INFO - root -   eval_accuracy_thresh after epoch 17: 0.9560185074806213: 
10/10/2019 09:34:35 - INFO - root -   eval_roc_auc after epoch 17: 0.9773163848507126: 
10/10/2019 09:34:35 - INFO - root -   eval_fbeta after epoch 17: 0.944444477558136: 
10/10/2019 09:34:35 - INFO - root -   eval_accuracy_multilabel after epoch 17: 0.9236111111111112: 
10/10/2019 09:34:35 - INFO - root -   lr after epoch 17: 3.7819373674036123e-06
10/10/2019 09:34:35 - INFO - root -   train_loss after epoch 17: 0.151565641808657
10/10/2019 09:34:35 - INFO - root -   

10/10/2019 09:36:11 - INFO - root -   Running evaluation
10/10/2019 09:36:11 - INFO - root -     Num examples = 144
10/10/2019 09:36:11 - INFO - root -     Batch size = 16
10/10/2019 09:36:12 - INFO - root -   eval_loss after epoch 18: 0.1640940871503618: 
10/10/2019 09:36:12 - INFO - root -   eval_accuracy_thresh after epoch 18: 0.9560185074806213: 
10/10/2019 09:36:12 - INFO - root -   eval_roc_auc after epoch 18: 0.9774468186022245: 
10/10/2019 09:36:12 - INFO - root -   eval_fbeta after epoch 18: 0.9398148655891418: 
10/10/2019 09:36:12 - INFO - root -   eval_accuracy_multilabel after epoch 18: 0.9236111111111112: 
10/10/2019 09:36:12 - INFO - root -   lr after epoch 18: 1.705290608732435e-06
10/10/2019 09:36:12 - INFO - root -   train_loss after epoch 18: 0.1469216418027142
10/10/2019 09:36:12 - INFO - root -   

10/10/2019 09:37:48 - INFO - root -   Running evaluation
10/10/2019 09:37:48 - INFO - root -     Num examples = 144
10/10/2019 09:37:48 - INFO - root -     Batch size = 16
10/10/2019 09:37:49 - INFO - root -   eval_loss after epoch 19: 0.16402660227484173: 
10/10/2019 09:37:49 - INFO - root -   eval_accuracy_thresh after epoch 19: 0.9560185074806213: 
10/10/2019 09:37:49 - INFO - root -   eval_roc_auc after epoch 19: 0.9773401000782603: 
10/10/2019 09:37:49 - INFO - root -   eval_fbeta after epoch 19: 0.9456018805503845: 
10/10/2019 09:37:49 - INFO - root -   eval_accuracy_multilabel after epoch 19: 0.9236111111111112: 
10/10/2019 09:37:49 - INFO - root -   lr after epoch 19: 4.300210136263111e-07
10/10/2019 09:37:49 - INFO - root -   train_loss after epoch 19: 0.14961972736098147
10/10/2019 09:37:49 - INFO - root -   

10/10/2019 09:39:26 - INFO - root -   Running evaluation
10/10/2019 09:39:26 - INFO - root -     Num examples = 144
10/10/2019 09:39:26 - INFO - root -     Batch size = 16
10/10/2019 09:39:27 - INFO - root -   eval_loss after epoch 20: 0.16401718391312492: 
10/10/2019 09:39:28 - INFO - root -   eval_accuracy_thresh after epoch 20: 0.9560185074806213: 
10/10/2019 09:39:28 - INFO - root -   eval_roc_auc after epoch 20: 0.9773282424644865: 
10/10/2019 09:39:28 - INFO - root -   eval_fbeta after epoch 20: 0.9456018805503845: 
10/10/2019 09:39:28 - INFO - root -   eval_accuracy_multilabel after epoch 20: 0.9236111111111112: 
10/10/2019 09:39:28 - INFO - root -   lr after epoch 20: 0.0
10/10/2019 09:39:28 - INFO - root -   train_loss after epoch 20: 0.1454938900783474
10/10/2019 09:39:28 - INFO - root -   

10/10/2019 09:39:28 - INFO - root -   Running evaluation
10/10/2019 09:39:28 - INFO - root -     Num examples = 144
10/10/2019 09:39:28 - INFO - root -     Batch size = 16
